debug = true
env = "dev"
[sourceDB]
host = "xxxxxxxxx"
port = 3306
username = "xxxxxx"
password = "xxxxxx"
charset = ""
# server id like a slave,同时还作为ID生成器,生成kafka消息的ID
serverID = 130
flavor = "mysql"
mysqldump = ""
# minimal items to be inserted in one bulk
bulkSize = 128
flushBulkTime = 200
skipNoPkTable = false
skipMasterData = false

#
[[sourceDB.sources]]
schema = "xxxxx"
tables = ["xxxxxx"]

#[[sourceDB.sources]]
#schema = "fonzie"
#tables = ["t", "t_[0-9]{4}", "tfield", "tfilter"]

#[[sourceDB.rule]]
#schema = "test"
#table = "t"
#index = "test"
#type = "t"

[http]
statAddr = "127.0.0.1:8000"
statPath = "/metrics"

[redis]
host = "127.0.0.1:6379"
password = "xxxxx"

db = 0
poolSize = 30
idleTimeout = 30
maxRetries = 5
dialTimeout = 10

# binlog在redis的TTL,目前没有用到这个功能
binlogTimeout = 900
binlogPrefix = "xxxxxxx"

# 分表分库做映射
[mapper]
schemas = ["xxxxxx"]

[kafka]
# kafka地址
brokers = ["xxxxxx:9092"]
# 设置kafka版本
version = "2.1.0"
# 跳过证书可信性检测
insecureSkipVerify = true
# 开启ssl访问
saslEnable = false
# 用户名
username = "xxxxx"
# 密码
password = "xxxxxx"
# ca证书
certFile = "ca-cert"




[kafka.producer]
# 等待服务器所有副本都保存成功后的响应, NoResponse: 0 WaitForLocal: 1 WaitForAll: -1,发送完数据需要leader和follow都确认
requiredAcks = 1
# 生产者投递影响消息在partitioner上的分布
# Manual: 只投递到partition 0,一般用于保序
# RoundRobin: rr轮训模式
# Random: 随机投递
# Hash: Hash投递
# ReferenceHash:
# 默认rr模式
PartitionerType = "default"
# 是否等待成功和失败后的响应,只有上面的RequireAcks设置不是NoReponse这里才有用.
returnSuccesses = true
returnErrors = true
# 消息投递失败重试次数
retryMax = 5



[[kafka.producer.headers]]
key = "xxxxxx"
value = "xxxxxx"

# 默认目标topic是表名,如果要重命名就可以在这里配置对应关系
[[kafka.producer.mapper]]
sourceTable = "xxx"
topic = "xxxx"
