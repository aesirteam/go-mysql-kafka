debug = true
env = "dev"

[sourceDB]
host = "192.168.9.159"
port = 3306
username = "root"
password = "Aic8Kee9"
charset = "utf8"
# server id like a slave,同时还作为ID生成器,生成kafka消息的ID
serverID = 130
flavor = "mysql"
mysqldump = ""
# minimal items to be inserted in one bulk
bulkSize = 128
flushBulkTime = 200
skipNoPkTable = false
skipMasterData = false
DataDir = "/tmp/mysql"
#
[[sourceDB.sources]]
schema = "rd_expert"
tables = ["gp_base_code"]

#[[sourceDB.sources]]
#schema = "fonzie"
#tables = ["t", "t_[0-9]{4}", "tfield", "tfilter"]

#[[sourceDB.rule]]
#schema = "test"
#table = "t"
#index = "test"
#type = "t"

[kafka]
# kafka地址
brokers = ["bootstrap.kafka:38443"]
# 设置kafka版本
version = "3.2.3"
# 跳过证书可信性检测
insecureSkipVerify = true
# 开启ssl访问
saslEnable = true
# 用户名
username = "kafka-scram-user"
# 密码
password = "m3j0HE4zijH7"
# ca证书
certFile = "ca-cert"
mechanism = "SCRAM-SHA-512"

[kafka.producer]
# 等待服务器所有副本都保存成功后的响应, NoResponse: 0 WaitForLocal: 1 WaitForAll: -1,发送完数据需要leader和follow都确认
requiredAcks = 1
# 生产者投递影响消息在partitioner上的分布
# Manual: 只投递到partition 0,一般用于保序
# RoundRobin: rr轮训模式
# Random: 随机投递
# Hash: Hash投递
# ReferenceHash:
# 默认rr模式
PartitionerType = "default"
# 是否等待成功和失败后的响应,只有上面的RequireAcks设置不是NoReponse这里才有用.
returnSuccesses = true
returnErrors = true
# 消息投递失败重试次数
retryMax = 5

#[[kafka.producer.headers]]
#key = "xxxxxx"
#value = "xxxxxx"

# 默认目标topic是表名,如果要重命名就可以在这里配置对应关系
#[[kafka.producer.mapper]]
#sourceTable = "xxx"
#topic = "xxxx"
